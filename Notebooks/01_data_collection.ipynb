{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "hJAmrlYN43x6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCDzUdpEVqMR",
        "outputId": "60049eaf-635e-4824-a00b-fa1d4d0418ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = \"/content/drive/MyDrive/stat_arb_pairs\"\n"
      ],
      "metadata": {
        "id": "iQtyVQ4537cP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Install & Import Libraries"
      ],
      "metadata": {
        "id": "gL0D0rfe48x3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance pandas numpy --quiet\n"
      ],
      "metadata": {
        "id": "OMiIes4A4Dnx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n"
      ],
      "metadata": {
        "id": "JAmZyMgF4GKa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Config: Universe, Paths and Dates"
      ],
      "metadata": {
        "id": "nKD9wD7d5A-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Asset universe (you can edit this later)\n",
        "UNIVERSE = [\n",
        "    # Index ETFs\n",
        "    \"SPY\", \"QQQ\", \"DIA\",\n",
        "    # Sector ETFs\n",
        "    \"XLF\", \"XLE\", \"XLK\", \"XLI\", \"XLP\",\n",
        "    # Large-cap stocks (good pairs candidates)\n",
        "    \"AAPL\", \"MSFT\", \"JPM\", \"BAC\", \"KO\", \"PEP\", \"XOM\", \"CVX\",\n",
        "]\n",
        "\n",
        "# 2) Date range (last 5 years)\n",
        "END_DATE = datetime.today()\n",
        "START_DATE = END_DATE - timedelta(days=5 * 365)\n",
        "\n",
        "# 3) Data folders (inside BASE_DIR defined earlier)\n",
        "DATA_DIR_RAW = os.path.join(BASE_DIR, \"data\", \"raw\")\n",
        "DATA_DIR_CLEANED = os.path.join(BASE_DIR, \"data\", \"cleaned\")\n",
        "\n",
        "os.makedirs(DATA_DIR_RAW, exist_ok=True)\n",
        "os.makedirs(DATA_DIR_CLEANED, exist_ok=True)\n",
        "\n",
        "print(\"Base dir:\", BASE_DIR)\n",
        "print(\"Saving raw data to:\", DATA_DIR_RAW)\n",
        "print(\"Saving cleaned data to:\", DATA_DIR_CLEANED)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwRqd4rr4K1a",
        "outputId": "2f9fe48b-b2d6-49ac-ecc0-94240ff4fe83"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base dir: /content/drive/MyDrive/stat_arb_pairs\n",
            "Saving raw data to: /content/drive/MyDrive/stat_arb_pairs/data/raw\n",
            "Saving cleaned data to: /content/drive/MyDrive/stat_arb_pairs/data/cleaned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Download Function"
      ],
      "metadata": {
        "id": "Aw3THvFX5GXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_price_data(tickers, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Download daily OHLCV data for a list of tickers using yfinance.\n",
        "    Returns a long-format DataFrame: [date, ticker, open, high, low, close, adj_close, volume]\n",
        "    \"\"\"\n",
        "    print(f\"Downloading data for {len(tickers)} tickers from Yahoo Finance...\")\n",
        "    data = yf.download(\n",
        "        tickers=tickers,\n",
        "        start=start_date.strftime(\"%Y-%m-%d\"),\n",
        "        end=end_date.strftime(\"%Y-%m-%d\"),\n",
        "        auto_adjust=False,\n",
        "        group_by=\"ticker\",\n",
        "        progress=True,\n",
        "    )\n",
        "\n",
        "    all_rows = []\n",
        "\n",
        "    for ticker in tickers:\n",
        "        if ticker not in data.columns.get_level_values(0):\n",
        "            print(f\"WARNING: No data for {ticker}, skipping.\")\n",
        "            continue\n",
        "\n",
        "        df_t = data[ticker].copy()\n",
        "        df_t[\"ticker\"] = ticker\n",
        "        df_t.rename(\n",
        "            columns={\n",
        "                \"Open\": \"open\",\n",
        "                \"High\": \"high\",\n",
        "                \"Low\": \"low\",\n",
        "                \"Close\": \"close\",\n",
        "                \"Adj Close\": \"adj_close\",\n",
        "                \"Volume\": \"volume\",\n",
        "            },\n",
        "            inplace=True,\n",
        "        )\n",
        "        df_t.index.name = \"date\"\n",
        "        all_rows.append(df_t.reset_index())\n",
        "\n",
        "    if not all_rows:\n",
        "        raise ValueError(\"No data downloaded for any ticker. Check ticker symbols or connection.\")\n",
        "\n",
        "    df_all = pd.concat(all_rows, axis=0, ignore_index=True)\n",
        "    df_all.sort_values([\"date\", \"ticker\"], inplace=True)\n",
        "\n",
        "    return df_all\n"
      ],
      "metadata": {
        "id": "4527a2Dm4Okd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Cleaning + Aligning Function"
      ],
      "metadata": {
        "id": "VwmZjGll5K26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_align_data(df):\n",
        "    \"\"\"\n",
        "    Clean the raw long-format DataFrame:\n",
        "    - Keep adj_close only (for now)\n",
        "    - Pivot to wide format with dates as index and tickers as columns\n",
        "    - Forward-fill/back-fill gaps\n",
        "    - Drop rows with any remaining NaNs\n",
        "    - Compute log returns\n",
        "    \"\"\"\n",
        "    print(\"Cleaning and aligning data...\")\n",
        "\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "    df.sort_values([\"date\", \"ticker\"], inplace=True)\n",
        "\n",
        "    # Wide format prices\n",
        "    price_wide = df.pivot(index=\"date\", columns=\"ticker\", values=\"adj_close\")\n",
        "\n",
        "    # Fill gaps\n",
        "    price_wide_ffill = price_wide.ffill().bfill()\n",
        "\n",
        "    # Keep only rows where all tickers have data\n",
        "    price_wide_cleaned = price_wide_ffill.dropna(how=\"any\")\n",
        "\n",
        "    # Log returns\n",
        "    log_returns = np.log(price_wide_cleaned / price_wide_cleaned.shift(1))\n",
        "    log_returns = log_returns.dropna(how=\"any\")\n",
        "\n",
        "    return price_wide_cleaned, log_returns\n"
      ],
      "metadata": {
        "id": "N9aUn03O4T7D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Save to CSV"
      ],
      "metadata": {
        "id": "GP5zuBNd5Rxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_data(raw_df, price_wide, log_returns):\n",
        "    raw_path_csv = os.path.join(DATA_DIR_RAW, \"prices_raw.csv\")\n",
        "    prices_csv = os.path.join(DATA_DIR_CLEANED, \"adj_close_wide.csv\")\n",
        "    returns_csv = os.path.join(DATA_DIR_CLEANED, \"log_returns_wide.csv\")\n",
        "\n",
        "    print(f\"Saving raw data to: {raw_path_csv}\")\n",
        "    raw_df.to_csv(raw_path_csv, index=False)\n",
        "\n",
        "    print(f\"Saving cleaned price data to: {prices_csv}\")\n",
        "    price_wide.to_csv(prices_csv)\n",
        "\n",
        "    print(f\"Saving log returns to: {returns_csv}\")\n",
        "    log_returns.to_csv(returns_csv)\n"
      ],
      "metadata": {
        "id": "gdTKgWWF4WSV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Running Pipeline"
      ],
      "metadata": {
        "id": "eeRDqrYS5Y_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== STAT ARB DATA PIPELINE: STEP 1 (DOWNLOAD + CLEAN) ===\")\n",
        "print(f\"Universe: {UNIVERSE}\")\n",
        "print(f\"Date Range: {START_DATE.date()} to {END_DATE.date()}\")\n",
        "\n",
        "raw_df = download_price_data(UNIVERSE, START_DATE, END_DATE)\n",
        "print(\"Downloaded rows:\", len(raw_df))\n",
        "\n",
        "prices_wide, log_returns = clean_and_align_data(raw_df)\n",
        "print(\"Cleaned prices shape:\", prices_wide.shape)\n",
        "print(\"Log returns shape:\", log_returns.shape)\n",
        "\n",
        "save_data(raw_df, prices_wide, log_returns)\n",
        "\n",
        "print(\"Done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9yHimvj4ZyL",
        "outputId": "7cd23162-9078-4334-94b5-347ec8874ab3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== STAT ARB DATA PIPELINE: STEP 1 (DOWNLOAD + CLEAN) ===\n",
            "Universe: ['SPY', 'QQQ', 'DIA', 'XLF', 'XLE', 'XLK', 'XLI', 'XLP', 'AAPL', 'MSFT', 'JPM', 'BAC', 'KO', 'PEP', 'XOM', 'CVX']\n",
            "Date Range: 2020-12-05 to 2025-12-04\n",
            "Downloading data for 16 tickers from Yahoo Finance...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  16 of 16 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded rows: 20064\n",
            "Cleaning and aligning data...\n",
            "Cleaned prices shape: (1254, 16)\n",
            "Log returns shape: (1253, 16)\n",
            "Saving raw data to: /content/drive/MyDrive/stat_arb_pairs/data/raw/prices_raw.csv\n",
            "Saving cleaned price data to: /content/drive/MyDrive/stat_arb_pairs/data/cleaned/adj_close_wide.csv\n",
            "Saving log returns to: /content/drive/MyDrive/stat_arb_pairs/data/cleaned/log_returns_wide.csv\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p6OdNyjS4b9C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}